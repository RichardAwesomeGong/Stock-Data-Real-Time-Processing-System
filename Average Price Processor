import sys
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
from kafka import KafkaProducer
import json
import logging
import time
import atexit
import socket
import win_inet_pton


logging.basicConfig(filename="log.txt")
logger=logging.getLogger("Stream_Processing")
logger.setLevel(logging.INFO)   #####


topic=""
target_topic=""
kafka_broker=""
kafkaProducer=None

def shut_down_hook(producer):
    producer.flush(10)
    producer.close(10)

    logger.info("Producer closed.")


#calcualte average price
def process(timeobj, rdd):    ###????  name must be rdd?
    numOfRecords=rdd.count()

    if numOfRecords==0:
        return
                                   # why converting?
    priceSum=rdd.map(lambda record: float(json.loads(record[1].decode("utf-8"))[0].get("LastTradePrice"))).reduce(lambda a, b: a+b)
    average=priceSum/numOfRecords

    logger.info("Number of records is %d and the average price is %f" % (numOfRecords, average))

    currentTime=time.time()
    data=json.dumps({"TimeStamp":currentTime, "AveragePrice":average})

    try:
        kafkaProducer.send(target_topic, value=data)
    except Exception as e:
        logger.warn("Failed to sent data to kafka.")



if __name__=="__main__":
    topic, target_topic, kafka_broker=sys.argv[1:]

    # create a SparkConext instance
    sc=SparkContext("local[2]", "StockAveragePrice")
    sc.setLogLevel("INFO")
    ssc=StreamingContext(sc, 5)

    # create a direct kafka stream to connect kafka cluster to ssc    ########why[]
    directKafkaStream=KafkaUtils.createDirectStream(ssc, [topic], {"metadata.broker.list": kafka_broker})   ####attention
    kafkaProducer=KafkaProducer(bootstrap_servers=kafka_broker)

    directKafkaStream.foreachRDD(process)

    atexit.register(shut_down_hook, kafkaProducer)

    ssc.start()
    ssc.awaitTermination()
