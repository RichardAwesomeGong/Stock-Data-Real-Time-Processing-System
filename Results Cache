# read from any kafka topic
# pubish to redis data structure

from kafka import KafkaConsumer

import redis
import logging
import atexit
import argparse
import win_inet_pton

logging.basicConfig()
logger=logging.getLogger("publish_to_Redis")
logger.setLevel(logging.DEBUG)


if __name__ == "__main__":

    # set up command line args
    parser = argparse.ArgumentParser()
    parser.add_argument("topic_name", help="the kafka topic to consume from")
    parser.add_argument("kafka_broker", help="the ip address of the kafka broker")
    parser.add_argument("redis_channel", help="the target redis channel to publish to")
    parser.add_argument("redis_host", help="the ip address of the server running redis")
    parser.add_argument("redis_port", help="the port number of redis in the redis host")


    args=parser.parse_args()
    topic_name=args.topic_name
    kafka_broker=args.kafka_broker
    redis_channel=args.redis_channel
    redis_host=args.redis_host
    redis_port=args.redis_port

    # kafka consumer
    kafka_consumer = KafkaConsumer(topic_name, bootstrap_servers=kafka_broker)

    # redis client
    redis_client = redis.StrictRedis(host=redis_host, port=redis_port)


    for msg in kafka_consumer:
        logger.info("New data is received from kafka %s" % msg.value)

        redis_client.publish(redis_channel,msg.value)
